import logging
import math
import os
from pathlib import Path
from typing import Dict, List, Sequence

import h5py
import lightning.pytorch as pl
import ray
import torch
from train.augmentations import PsdEstimator, WaveformProjector
from train.data.utils import ParameterTransformer

from ml4gw.dataloading import Hdf5TimeSeriesDataset
from ml4gw.transforms import ChannelWiseScaler, Whiten
from ml4gw.utils.slicing import unfold_windows

Tensor = torch.Tensor
Distribution = torch.distributions.Distribution


class BaseDataset(pl.LightningDataModule):
    """
    Base Dataset for PE dataloading.

    Subclasses should inherit and override the following methods:

    - fit_scaler: fit the standard scaler to parameters
    - get_val_waveforms: load validation waveforms and parameters
    - sample_waveforms: sample waveforms during training

    The goal is to support subclasses for
    1. Generating waveforms on the fly in torch (see generator.py)
    2. Loading waveforms from disk

    """

    def __init__(
        self,
        data_dir: Path,
        sample_rate: float,
        val_stride: float,
        kernel_length: float,
        fduration: float,
        psd_length: float,
        highpass: float,
        batches_per_epoch: int,
        batch_size: int,
        ifos: List[str],
        parameter_transformer: ParameterTransformer,
        inference_params: List[str],
        dec: Distribution,
        psi: Distribution,
        phi: Distribution,
        fftlength: float = 2,
        device: str = "cpu",
        min_valid_duration: float = 10000,
    ):
        super().__init__()
        self.save_hyperparameters()
        self.num_ifos = len(ifos)
        self.data_dir = data_dir
        self.train_fnames, self.val_fnames = self.train_val_split()
        self.inference_params = inference_params
        self.parameter_transformer = parameter_transformer
        self.sample_rate = sample_rate
        self.sample_length = self.calc_sample_length()

        # these are modules that require our data to be
        # downloaded first, either for loading signals
        # or to infer sample rate, so wait to construct
        # them until self.setup
        self.waveform_sampler = None
        self.whitener = None
        self.projector = None
        self.psd_estimator = None
        self._on_device = False

    # ================================================ #
    # Distribution utilities
    # ================================================ #
    def get_local_device(self):
        """
        Get the device string for the device that
        we're actually training on so that we can move
        our transforms on to it because Lightning won't
        do this for us in a DataModule. NOTE!!! This
        method is currently incorrect during ray distributed
        training, but keeping it for posterity in the hopes
        that we can eventually get rid of self._move_to_device
        """
        if not self.trainer.device_ids:
            return "cpu"
        elif len(self.trainer.device_ids) == 1:
            return f"cuda:{self.trainer.device_ids[0]}"
        else:
            _, rank = self.get_world_size_and_rank()
            if ray.is_initialized():
                device_ids = ray.train.torch.get_device()
                if isinstance(device_ids, list):
                    return device_ids[rank]
                return device_ids
            else:
                device_id = self.trainer.device_ids[rank]
                return f"cuda:{device_id}"

    def get_world_size_and_rank(self) -> tuple[int, int]:
        """
        Name says it all, but generalizes to the case
        where we aren't running distributed training.
        """
        if not torch.distributed.is_initialized():
            return 1, 0
        else:
            world_size = torch.distributed.get_world_size()
            rank = torch.distributed.get_rank()
            return world_size, rank

    def get_logger(self, world_size, rank):
        logger_name = "AframeDataset"
        if world_size > 1:
            logger_name += f":{rank}"
        return logging.getLogger(logger_name)

    # ================================================ #
    # Re-paramterizing some attributes
    # ================================================ #

    def calc_sample_length(self) -> float:
        """Length of samples generated by datasets in seconds"""
        return (
            self.hparams.kernel_length
            + self.hparams.fduration
            + self.hparams.psd_length
        )

    @property
    def strain_dim(self):
        """
        Size of strain tensor timeseries
        the nerual network will analyze
        """
        return int(self.hparams.kernel_length * self.sample_rate)

    @property
    def num_params(self):
        """Number of parameters inference is performed on"""
        return len(self.hparams.inference_params)

    @property
    def val_batch_size(self):
        """Use larger batch sizes when we don't need gradients."""
        return int(4 * self.hparams.batch_size)

    @property
    def background_fnames(self):
        """List of background files used for both training and validation"""
        background_dir = self.data_dir / "train" / "background"
        fnames = list(background_dir.glob("*.hdf5"))
        return fnames

    @property
    def test_fnames(self):
        """List of background files used for testing trained model"""
        test_dir = self.data_dir / "test" / "background"
        fnames = list(test_dir.glob("*.hdf5"))
        return fnames

    def train_val_split(self) -> Sequence[str]:
        """
        Split background files into training and validation sets
        based on the requested duration of the validation set
        """
        fnames = sorted(self.background_fnames)
        durations = [int(fname.stem.split("-")[-1]) for fname in fnames]
        valid_fnames = []
        valid_duration = 0
        while valid_duration < self.hparams.min_valid_duration:
            fname, duration = fnames.pop(-1), durations.pop(-1)
            valid_duration += duration
            valid_fnames.append(str(fname))

        train_fnames = fnames
        return train_fnames, valid_fnames

    # ================================================ #
    # Helper utilities for preprocessing
    # ================================================ #
    def transform(self, parameters: Dict[str, Tensor]):
        """
        Make transforms to parameters before scaling
        and performing training/inference.
        For example, taking logarithm of hrss
        """
        return self.parameter_transformer(parameters)

    def scale(self, parameters):
        """
        Apply standard scaling to transformed parameters
        """
        parameters = parameters.transpose(1, 0)
        scaled = self.standard_scaler(parameters)
        scaled = scaled.transpose(1, 0)
        return scaled

    # ================================================ #
    # Utilities for initial data loading and preparation
    # ================================================ #
    def setup(self, stage: str) -> None:
        world_size, rank = self.get_world_size_and_rank()
        self._logger = self.get_logger(world_size, rank)

        # infer sample rate directly from background data
        with h5py.File(self.train_fnames[0], "r") as f:
            sample_rate = 1 / f[self.hparams.ifos[0]].attrs["dx"]
            assert sample_rate == self.hparams.sample_rate

        self._logger.info("Building augmentation modules")
        self.build_modules()

        self._logger.info("Building validation and testing datasets")
        # load validation waveforms, parameters, and background;
        # get_val_waveforms should be implemented by subclassses
        self.val_waveforms, self.val_parameters = self.get_val_waveforms()
        self.val_background = self.load_background(self.val_fnames)

        # load testing background
        self.test_background = self.load_background(self.test_fnames)

        return world_size, rank

    def fit_scaler(self, scaler: ChannelWiseScaler) -> ChannelWiseScaler:
        """
        Method subclasses should define
        to fit standard scaler to parameters
        """
        raise NotImplementedError

    def get_val_waveforms(self):
        """Method for constructing validation waveforms and parameters"""
        raise NotImplementedError

    def load_background(self, fnames: Sequence[str]):
        background = []
        for fname in fnames:
            data = []
            with h5py.File(fname, "r") as f:
                for ifo in self.hparams.ifos:
                    back = f[ifo][:]
                    data.append(torch.tensor(back, dtype=torch.float32))
            data = torch.stack(data)
            background.append(data)
        return background

    def build_modules(self):
        """
        Helper utility in case we ever want to construct
        this dataset on its own.
        """
        window_length = self.hparams.kernel_length + self.hparams.fduration
        fftlength = self.hparams.fftlength or window_length
        self.psd_estimator = PsdEstimator(
            window_length,
            self.hparams.sample_rate,
            fftlength,
            fast=self.hparams.highpass is not None,
            average="median",
        )
        self.whitener = Whiten(
            self.hparams.fduration,
            self.hparams.sample_rate,
            self.hparams.highpass,
        )

        # build standard scaler object and fit to parameters
        # defined by the scaler_parameters attribute subclasses
        # will define
        standard_scaler = ChannelWiseScaler(self.num_params)
        self.standard_scaler = self.fit_scaler(standard_scaler)

        self.projector = WaveformProjector(
            self.hparams.ifos, self.hparams.sample_rate
        )

    # ================================================ #
    # Utilities for doing augmentation/preprocessing
    # after tensors have been transferred to GPU
    # ================================================ #

    def _move_to_device(self, device):
        """
        This is dumb, but I genuinely cannot find a way
        to ensure that our transforms end up on the target
        device (see NOTE under self.get_local_device), so
        here's a lazy workaround to move our transforms once
        we encounter the first on-device tensor from our dataloaders.
        """
        if self._on_device:
            return
        for item in self.__dict__.values():
            if isinstance(item, torch.nn.Module):
                item.to(device)
        self._on_device = True

    def on_after_batch_transfer(self, batch, _):
        """
        This is a method inherited from the DataModule
        base class that gets called after data returned
        by a dataloader gets put on the local device,
        but before it gets passed to the LightningModule.
        Use this to do on-device augmentation/preprocessing,
        or injecting of waveforms.
        """

        if self.trainer.training:

            [X] = batch
            self._move_to_device(X)
            strain, parameters = self.inject(X)

        elif self.trainer.validating or self.trainer.sanity_checking:

            (strain, parameters) = batch

        return strain, parameters

    def sample_waveforms(self, device):
        """
        Method subclasses should override
        to define how waveforms are sampled during training
        """
        raise NotImplementedError

    def sample_extrinsic(self, N: int, device: str):
        """
        Helper function for sampling extrinsic parameters
        used to project waveforms
        """
        dec = self.hparams.dec.sample((N,)).to(device)
        phi = self.hparams.phi.sample((N,)).to(device)
        psi = self.hparams.psi.sample((N,)).to(device)
        return dec, phi, psi

    def inject(self, X):
        device = X.device
        # inject waveforms into every kernel
        N = len(X)

        # split and estimate psd and whiten data
        X, psds = self.psd_estimator(X)

        # infer kernel size from background
        kernel_size = X.shape[-1]

        # sample waveforms, extrinsic
        # parameters, and project onto ifos
        cross, plus, parameters = self.sample_waveforms(N, device=device)
        dec, phi, psi = self.sample_extrinsic(N, device=device)
        waveforms = self.projector(dec, phi, psi, cross=cross, plus=plus)

        # append extrinisc parameters to parameters
        parameters.update({"dec": dec, "phi": phi, "psi": psi})

        # downselect to requested inference parameters
        parameters = {
            k: v for k, v in parameters.items() if k in self.inference_params
        }

        # make any requested parameter transforms
        parameters = self.transform(parameters)
        parameters = [torch.Tensor(v) for v in parameters.values()]
        parameters = torch.vstack(parameters).T

        # calculate the fixed location
        # where waveform T_c will placed
        # TODO: allow jitter and infer T_c
        center = waveforms.shape[-1] // 2
        start = center - (kernel_size // 2)
        stop = center + (kernel_size // 2)
        # inject waveforms and whiten
        waveforms = waveforms[:, :, start:stop]
        X += waveforms
        X = self.whitener(X, psds)

        # scale parameters
        parameters = self.scale(parameters)

        return X, parameters

    def val_dataloader(self):
        """
        Method that constructs validation batches from a background
        timeseries and sequence of waveforms.
        """
        background = self.val_background[0]
        _, rank = self.get_world_size_and_rank()
        waveforms, parameters = self.val_waveforms, self.val_parameters

        # size of psd data + filter settle in + kernel
        sample_size = int(self.hparams.sample_rate * self.sample_length)

        # size of filter settle in + kernel
        kernel_size = int(
            (self.hparams.kernel_length + self.hparams.fduration)
            * self.hparams.sample_rate
        )

        # reduce waveforms to just filter settle in + kernel size
        center = waveforms.shape[-1] // 2
        start = center - (kernel_size // 2)
        stop = center + (kernel_size // 2)
        waveforms = waveforms[:, :, start:stop]

        # calculated number of background kernels we can make
        # depending on the stride size and length of validation set
        stride_size = int(self.hparams.val_stride * self.hparams.sample_rate)
        num_kernels = (background.shape[-1] - kernel_size) // stride_size + 1
        num_kernels = int(num_kernels)

        # Create pre-computed kernels of pure background
        # slice our background so that it has an integer number of
        # windows, then add dummy dimensions since unfolding only
        # works on 4D tensors.
        # Add a shift corresponding to current rank (if distributed)
        # to add more diversity of background data in validation
        shift = int(self.hparams.sample_rate * rank)

        background = background[
            :, : int(num_kernels * stride_size + sample_size + shift)
        ]
        background = background[:, shift:]
        background = unfold_windows(
            background, sample_size, stride=stride_size
        )

        # create repeats of background kernels if we need any
        repeats = math.ceil(len(waveforms) / len(background))
        background = background.repeat(repeats, 1, 1)
        background = background[: len(waveforms)]

        # calculate psd, inject and whiten
        X, psds = self.psd_estimator(background)
        X += waveforms
        X = self.whitener(X, psds)

        # downselect to requested inference parameters and
        # stack into a single tensor, and scale
        parameters = {
            k: v for k, v in parameters.items() if k in self.inference_params
        }
        parameters = [torch.Tensor(v) for v in parameters.values()]
        parameters = torch.vstack(parameters).T
        parameters = self.scale(parameters)

        dataset = torch.utils.data.TensorDataset(X, parameters)
        return torch.utils.data.DataLoader(
            dataset,
            pin_memory=True,
            batch_size=self.val_batch_size,
        )

    def train_dataloader(self):
        dataset = Hdf5TimeSeriesDataset(
            self.train_fnames,
            channels=self.hparams.ifos,
            kernel_size=int(self.sample_rate * self.sample_length),
            batch_size=self.hparams.batch_size,
            batches_per_epoch=self.hparams.batches_per_epoch,
            coincident=False,
        )

        pin_memory = isinstance(
            self.trainer.accelerator, pl.accelerators.CUDAAccelerator
        )
        local_world_size = len(self.trainer.device_ids)
        num_workers = min(6, int(os.cpu_count() / local_world_size))
        dataloader = torch.utils.data.DataLoader(
            dataset, num_workers=num_workers, pin_memory=pin_memory
        )
        return dataloader
